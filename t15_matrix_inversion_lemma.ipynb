{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 15.  The Matrix Inversion Lemma\n",
    "Author: Nicholas Kohls - kohlsn@gmail.com\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "<!-- Give a high level (i.e., no equations) explanation of the topic, and why it is important. -->\n",
    "\n",
    "The matrix inversion lemma is also known as the Sherman-Morrison formula or sometimes the Woodbury's idenity.\n",
    "\n",
    "The matrix inversion lemma says that the inverse of a rank-k correction of some matrix can be computed by doing a rank-k correction to the inverse of the original matrix. In other words, the lemma gives a method of finding the inverse of an updated matrix without having to directly compute the inverse of the entire updated matrix. This is possible if the original matrix's inverse in known. A common application is when something of low rank added to something of full rank and the inverse needs to be known.\n",
    "\n",
    "Let $B^{-1} = (A+UCV)^{-1}$\n",
    ",the matrix inversion lemma finds $B^{-1}$ without computing it directly. If the size of the matrix is a large number, computing $B^{-1}$ directly is computational heavy. If this is the case finding $B^{-1}$ using a less computational heavy method is desired. The matrix inverstion lemma finds $B^{-1}$ using the known $A^{-1}$. The matrix inversion lemma proves that the inverse of the updated Matrix does not need to recomputed directly.\n",
    "\n",
    "The matrix inversion lemma can be used to provide an update to the inverse ot the Grammian matrix in a least-squares problem, to produce a simple version of what is known as the recursive least-squares (RLS) adaptive filter (explored in the next section).\n",
    "\n",
    "The matrix inversion leema is applied in the Kalman filter and recursive least squares methods, to replace the parametric solution, requiring inversion of a state vector sized matrix, with a condition equations based solution. In case of the Kalman filter this matrix has the dimensions of the vector of observations, i.e., as small as 1 in case only one new observation is processed at a time. This significantly speeds up the often real time calculations of the filter.\n",
    "\n",
    "Essentailly this lemma is used in many engineering practices where input is constnaly being fed and the inverse of the new data set needs to be computed. This is common in control systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the theory\n",
    "\n",
    "<!-- Give a detailed discussion (i.e., equations galore) of the topic.Â   -->\n",
    "<!-- The emphasis here is clarity for future students learning the topic. -->\n",
    "\n",
    "$\\textbf{Matrix Inversion Lemma (General Form):}$\n",
    "\n",
    "The Woodbury matrix identity gives the inverse of an $n\\times n$ square matrix $A$ modified by a perturbation term $UCV$ \n",
    "\n",
    "Let $B = A+UCV$\n",
    "where $A$ is $n\\times n$, $U$ is $n\\times k$, $C$ is $k\\times k$, and $V$ is $k\\times n$\n",
    "\n",
    "Matrix Inversion Lemma:\n",
    "$B^{-1} = (A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1})VA^{-1}$\n",
    "\n",
    "\n",
    "$\\textbf{Direct Proof:}$\n",
    "\n",
    "The formula can be proven by checking that <math>(A + UCV)</math> times its alleged inverse on the right side of the matrix inversion lemma gives the identity matrix:\n",
    "\n",
    "<math>\\begin{align}\n",
    "      & \\left(A + UCV \\right) \\left[ A^{-1} - A^{-1}U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1} \\right] \\\\\n",
    "  ={} & \\left\\{ I - U\\left(C^{-1} + VA^{-1}U \\right)^{-1}VA^{-1} \\right\\} + \\left\\{ UCVA^{-1} - UCVA^{-1}U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1} \\right\\} \\\\\n",
    "  ={} & \\left\\{ I + UCVA^{-1} \\right\\} - \\left\\{ U\\left(C^{-1} + VA^{-1}U \\right)^{-1}VA^{-1} + UCVA^{-1}U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1} \\right\\} \\\\\n",
    "  ={} & I + UCVA^{-1} - \\left(U + UCVA^{-1}U\\right) \\left(C^{-1} + VA^{-1}U\\right)^{-1}VA^{-1} \\\\\n",
    "  ={} & I + UCVA^{-1} - UC \\left(C^{-1} + VA^{-1}U\\right) \\left(C^{-1} + VA^{-1}U\\right)^{-1}VA^{-1} \\\\\n",
    "  ={} & I + UCVA^{-1} - UCVA^{-1} \\\\\n",
    "  ={} & I\n",
    "\\end{align}</math>\n",
    "\n",
    "\n",
    "$\\textbf{Derivation Proof:}$\n",
    "Two identities are needed to derive the matrix inversion lemma.\n",
    "\n",
    "Idenity 1: \n",
    "\n",
    "$(I+P)^{-1} = (I+P)^{-1}(I+P-P) = I - (I+P)^{-1}P$\n",
    "\n",
    "Idenity 2: \n",
    "\n",
    "$P + PQP = P(I+QP) = (I+PQ)P(I+PQ)^{-1}P = P(I+QP)^{-1}$\n",
    "\n",
    "Proof: \n",
    "\n",
    "Use Idenity 1\n",
    "$(A + UCV)^{-1}\\\\\n",
    "= (A[I+A^{-1}UCV])^{-1}\\\\\n",
    "= [I+A^{-1}UCV]^{-1}A^{-1}\\\\\n",
    "= [I-(I+A^{-1}UCV)^{-1}A^{-1}UCV]A^{-1}$\n",
    "$= A^{-1} - (I + A^{-1}UCV)^{-1}A^{-1}UCVA^{-1}$\n",
    "\n",
    "Repeatly using Idenity 2 produces the following:\n",
    "$(A + UCV)^{-1}\\\\\n",
    "=A^{-1}-(I + A^{-1}UCV)^{-1}A^{-1}UCVA^{-1} \\\\\n",
    "=A^{-1}-A^{-1}(I + UCVA^{-1})^{-1}A^{-1}UCVA^{-1}\\\\\n",
    "=A^{-1}-A^{-1}U(I + CVA^{-1}U)^{-1}A^{-1}CVA^{-1}\\\\\n",
    "=A^{-1}-A^{-1}UC(I + VA^{-1}UC)^{-1}A^{-1}VA^{-1}\\\\\n",
    "=A^{-1}-A^{-1}UCV(I + A^{-1}UCV)^{-1}A^{-1}A^{-1}\\\\\n",
    "=A^{-1}-A^{-1}UCVA^{-1}(I + UCVA^{-1})^{-1}$\n",
    "\n",
    "Take one of iterations of the last step to find lemma\n",
    "$(A + UCV)^{-1}\\\\\n",
    "=A^{-1} - A^{-1}U(I + CVA^{-1}U)^{-1}A^{-1}CVA^{-1}\\\\\n",
    "=A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1})VA^{-1}\n",
    "$\n",
    "\n",
    "\n",
    "$\\textbf{Rank one updates:}$\n",
    "\n",
    "One of the simplest changes that can be performed on a matrix is a so-called rank one update. \n",
    "\n",
    "If $U$ and $V$ are $k\\times 1$ column vectors $\\bf{u}$ and $\\bf{v}$, the perturbation on $A$ is a rank one update.\n",
    "\n",
    "The reason why the transformation is called rank one is that the rank of the $k\\times k$ matrix $\\bf{uv}^{H}$ is equal to 1 (because a single vector, $u$, spans all the columns of $\\bf{uv}^{H}$). \n",
    "\n",
    "\n",
    "Matrix inversion lemma for rank one updates:\n",
    "\n",
    "The matrix inversion lemma with the column vectors $\\bf{u}$ and $\\bf{v}$ is simplified from its general form.\n",
    "\n",
    "Where $B = A+\\bf{u}\\bf{v}^{H}$ and $\\bf{u}$ and $\\bf{v}$ are $k\\times 1$ column vectors.\n",
    "\n",
    "$B^{-1} = (A+\\bf{u}\\bf{v}^{H})^{-1} = A^{-1} - \\frac{A^{-1}\\bf{u}\\bf{v}^{H}A^{-1}}{1+\\bf{v}^{H}A^{-1}\\bf{u}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Numerical Example\n",
    "\n",
    "Here is some simple python code that shows the concepts of matrix inversion lemma. This code uses the numpy inverse of the update matrix and compares it with matrix inversion lemma version of the updated inverted matrix. The difference should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix inversion lemma, rank one update\n",
    "import numpy as np\n",
    "\n",
    "def matrix_inversion_lemma(A_Inv,u,v):\n",
    "    v_transpose = np.transpose(v)\n",
    "    uvt = np.dot(u, v_transpose)\n",
    "    numerator = np.dot(np.dot(A_Inv, uvt), A_Inv)\n",
    "    denominator = np.add(1, np.dot(np.dot(v_transpose, A_Inv), u))\n",
    "    B_Inv_lemma = np.subtract(A_Inv, np.divide(numerator, denominator))\n",
    "    return B_Inv_lemma\n",
    "\n",
    "# Known Values:\n",
    "A = np.array([[1, 2, 3, 4],[1,-2,6,2],[4,2,1,0],[8,4,3,1]]) #Random values in 4x4\n",
    "A_Inv = np.linalg.inv(A) # Inverse of A\n",
    "# Updated Values\n",
    "u = np.array([1,4,3,1]) #Random values\n",
    "u = np.expand_dims(u, axis=1)\n",
    "v = np.array([3,-1,1,2]) #Random values\n",
    "v = np.expand_dims(v, axis=1)\n",
    "\n",
    "# Computational Heavy Matrix/ Old Method\n",
    "B = np.add(A,np.dot(u, np.transpose(v))) # Updated Matrix\n",
    "B_Inv = np.linalg.inv(B)  #Calculate inverse\n",
    "\n",
    "# Matrix Inversion Lemma Method:\n",
    "B_Inv_lemma = matrix_inversion_lemma(A_Inv,u,v)\n",
    "\n",
    "#Find the difference between the Inverse caluclated directly and the inverse found uisng the Matrix inversion lemma\n",
    "Diff = np.abs(np.subtract(B_Inv,B_Inv_lemma).round(10))\n",
    "print(Diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Engineering Application\n",
    "\n",
    "<!-- Provide a more sophisticated example showing one engineering example of the topic, complete with python code. -->\n",
    "\n",
    "A missle is travelling through the air and uses sensors to determine its position. The sensor data is stored in matricies and measures how far the missile has moved. Every nanosecond there is a new reading from the sensor. It is computationally expensive to recompute the whole matrix of the readings for course correction, calibrations, etc. The computer in the missile can not keep up with the pace of the sensor readings if the whole matrix is recomputed everytime.\n",
    "\n",
    "The position of the missle is a linear system problem where $Ax = b$. Where $x$ is the position of the missle. If we have a change in Matrix A from a sensor reading we get $(A+\\bf{u}\\bf{v}^{T})x = b$\n",
    "\n",
    "Using the matrix inverstion lemma we find that $x = (A+\\bf{u}\\bf{v}^{T})^{-1}b = A^{-1}b - \\frac{A^{-1}\\bf{u}\\bf{v}^{T}A^{-1}}{1+\\bf{v}^{T}A^{-1}\\bf{u}}b$\n",
    "\n",
    "\n",
    "Lets say that the original matrix $A$ = \n",
    "\\begin{pmatrix}\n",
    "2 & 4 & -2\\\\\n",
    "4 & 9 & -3\\\\\n",
    "-2 & -3 & 7\n",
    "\\end{pmatrix}\n",
    "and the updated matrix $B$ = \n",
    "\\begin{pmatrix}\n",
    "2 & 4 & -2\\\\\n",
    "4 & 9 & -3\\\\\n",
    "-2 & -1 & 7\n",
    "\\end{pmatrix}\n",
    "\n",
    "$B = A + \\bf{u}\\bf{v}^{T}$\n",
    "\n",
    "The $a_{3,2}$ element changed form -3 to -1\n",
    "\n",
    "This means $\\bf{u}$ and $\\bf{v}$ can be:\n",
    "\n",
    "$\\bf{u} = $\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "\n",
    "$\\bf{v} = $\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "\n",
    "Lets say the updated linear system is \n",
    "\n",
    "$\\left(\\begin{matrix}\n",
    "2 & 4 & -2\\\\\n",
    "4 & 9 & -3\\\\\n",
    "-2 & -1 & 7\n",
    "\\end{matrix} \\right)\n",
    "%\n",
    "\\left( \\begin{matrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\n",
    "\\end{matrix} \\right) = \n",
    "\\left( \\begin{matrix}\n",
    "2\\\\\n",
    "8\\\\\n",
    "10\n",
    "\\end{matrix} \\right)$\n",
    "\n",
    "In this example $A^{-1}$ is already known.\n",
    "\n",
    "$A^{-1} = $\n",
    "\\begin{pmatrix}\n",
    "6.75 & -2.75 & 0.75\\\\\n",
    "-2.75 & 1.25 & -0.25\\\\\n",
    "0.75 & -0.25 & 0.25\n",
    "\\end{pmatrix}\n",
    "\n",
    "The code below finds the position vector $x$ without inverting the matrix $B^{-1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.]\n",
      " [ 4.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def matrix_inversion_lemma(A_Inv,u,v):\n",
    "    v_transpose = np.transpose(v)\n",
    "    uvt = np.dot(u, v_transpose)\n",
    "    numerator = np.dot(np.dot(A_Inv, uvt), A_Inv)\n",
    "    denominator = np.add(1, np.dot(np.dot(v_transpose, A_Inv), u))\n",
    "    B_Inv_lemma = np.subtract(A_Inv, np.divide(numerator, denominator))\n",
    "    return B_Inv_lemma\n",
    "\n",
    "# Known Values:\n",
    "A = np.array([[2,4,-2],[4,9,-3],[-2,-3,7]])\n",
    "A_Inv = np.array([[6.75,-2.75,0.75],[-2.75,1.25,-0.25],[0.75,-0.25,0.25]])\n",
    "b = np.array([2,8,10])\n",
    "b = np.expand_dims(b, axis=1)\n",
    "# Updated Values\n",
    "u = np.array([0,0,2])\n",
    "u = np.expand_dims(u, axis=1)\n",
    "v = np.array([0,1,0]) #Random values\n",
    "v = np.expand_dims(v, axis=1)\n",
    "\n",
    "# Matrix Inversion Lemma Method:\n",
    "B_Inv_lemma = matrix_inversion_lemma(A_Inv,u,v)\n",
    "x = np.dot(B_Inv_lemma,b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the matrix $A$ = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & -2\\\\\n",
    "4 & 4 & -3\\\\\n",
    "-1 & -3 & -2\n",
    "\\end{pmatrix}\n",
    "\n",
    "Matrix $A$ is updated into matrix $B$ = \n",
    "\\begin{pmatrix}\n",
    "2 & 0 & -2\\\\\n",
    "8 & 4 & -3\\\\\n",
    "-2 & -3 & -2\n",
    "\\end{pmatrix}\n",
    "\n",
    "$A^{-1}$ = \n",
    "\\begin{pmatrix}\n",
    "17 & -6 & -8\\\\\n",
    "-11 & 4 & 5\\\\\n",
    "8 & -3 & -4\n",
    "\\end{pmatrix}\n",
    "\n",
    "\n",
    "Find $B^{-1}$ using the matrix inversion lemma \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
